{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Aim\n",
        "=====\n",
        "\n",
        "The aim of this session is to familiarize yourselves with decision trees, as well as their bagged and boosted versions, which are famous ML models for precision medicine problems due to their high interpretability. For the purposes of this session, we are going to use a publicly available dataset, the heart failure prediction dataset that can be found [here](zhttps://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\n",
        "\n",
        "In the last part of this session, we will explore ways to improve your overall classification pipeline that are not specific to decision trees and ensembles based on trees but are important to know.\n",
        "\n",
        "Please note that we will not perform exploratory data analysis as you have already done this for this dataset in a previous session.\n",
        "\n",
        "Dataset:\n",
        "========\n",
        "The dataset contains data from 918 individuals and was created with the aim of identifying people who have cardiovascular disease (CVD) or are at risk of developing CVD. In total, 11 features are available:\n",
        "\n",
        "1. Age: age of the patient [years]\n",
        "2. Sex: sex of the patient [M: Male, F: Female]\n",
        "3. ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, 4. NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
        "4. RestingBP: resting blood pressure [mm Hg]\n",
        "5. Cholesterol: serum cholesterol [mm/dl]\n",
        "6. FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
        "7. RestingECG: resting electrocardiogram results â€“ measures the electrical activity of the heart [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n",
        "10. Oldpeak: ST [Numeric value measured in depression]\n",
        "11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
        "Additionally the column HeartDisease provides the output class [1: heart disease, 0: Normal]\n",
        "\n",
        "The dataset was adopted from: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
        "\n",
        "Author: Polyxeni Gkontra, Machine Learning for Precision Medicine, MBDS, 2024"
      ],
      "metadata": {
        "id": "fo30wimCxvhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation\n",
        "============"
      ],
      "metadata": {
        "id": "AYCejKJIZWZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries"
      ],
      "metadata": {
        "id": "iykhj5-ex2ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import io\n",
        "# Libraries related to classification\n",
        "from sklearn.metrics import classification_report, roc_curve, ConfusionMatrixDisplay, confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, MinMaxScaler"
      ],
      "metadata": {
        "id": "Ahxe41Yxx6AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the file with the data to colab"
      ],
      "metadata": {
        "id": "Re0__zFY2MFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Load the file - a window will prompt to choose from your local system\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "CvJmeGDV2LXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the .csv file with the patient information. Then create a dataframe X that contains only the features and another one that contains the labels. Useful: [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function from pandas"
      ],
      "metadata": {
        "id": "uxuUFB9ryFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv file\n",
        "data = pd.read_csv(io.BytesIO(uploaded['hd_data.csv']))"
      ],
      "metadata": {
        "id": "A6xYJLEx0498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column name that contains the class information\n",
        "label = 'HeartDisease'\n",
        "# Dataframe with the labels\n",
        "Y =pd.DataFrame(columns=[label])\n",
        "Y[label] = data[label].copy()\n",
        "# Drop the column with the classes from the X data so as to create a dataframe containing only features\n",
        "X = data.drop(label, axis=1)"
      ],
      "metadata": {
        "id": "Cr2od-3hx-aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the number of patients in each class and create a relevant plot. Useful: [countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html)"
      ],
      "metadata": {
        "id": "lpFiWnSNyTtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of individuals in each class\n",
        "\n",
        "# Create a countplot with the number of patients per category\n"
      ],
      "metadata": {
        "id": "2o4E646myU-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing values (Helpful: Function [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html) from [pandas](https://pandas.pydata.org/docs/))"
      ],
      "metadata": {
        "id": "rFm_MnL931up"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRsX2mM-3yiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic statistics (Helpful: Methods [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html), [info](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) from pandas)"
      ],
      "metadata": {
        "id": "JsH64Xfs36WP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7nBDAGj3745"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into training (80%) and testing (20%). We will leave the testing set aside and further split the training into training and validation. Useful: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ],
      "metadata": {
        "id": "B09y_SOryamq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test ="
      ],
      "metadata": {
        "id": "pfPuB6uhydPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check first rows of X_train"
      ],
      "metadata": {
        "id": "zX6kODf90S5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "qrl7kr990Z2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform categorical variables using one hot-key encoding for sex and ordinal encoding for the rest"
      ],
      "metadata": {
        "id": "mDji11j2LOjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type of the features\n",
        "X_train.dtypes"
      ],
      "metadata": {
        "id": "ih-6lqa-LluL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the categorical features and their indexes as well as the index of the sex attribute to treat it differently during encoding."
      ],
      "metadata": {
        "id": "cbh98fcdNA-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical features\n",
        "num_features = list(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
        "# Indices of numerical features\n",
        "numerical_idx = [loc for loc, key in enumerate(X_train.columns) if key in num_features]\n",
        "# Indices of columns with categorical data\n",
        "categorical_idx = list(set(range(0,X_train.shape[1])) - set(numerical_idx))\n",
        "# Index of sex attribute\n",
        "sex_idx = [loc for loc, key in enumerate(X_train.columns) if key == 'Sex']\n",
        "# Delete sex from the indices of categorical features\n",
        "categorical_idx.remove(sex_idx[0])\n",
        "# Names of categorical features\n",
        "cat_features = X_train.columns[categorical_idx]\n",
        "# Print all features\n",
        "print(\"All available features: \", X_train.columns)\n",
        "# Print numerical features\n",
        "print(\"Numerical features: \", num_features)\n",
        "# Print categorical features\n",
        "print(\"Categorical features: \", cat_features)"
      ],
      "metadata": {
        "id": "5HuCD-5-NAGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the sex attribute by means of one-hot key encoding and the rest using ordinal encoding. Useful: [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn-preprocessing-ordinalencoder)"
      ],
      "metadata": {
        "id": "gzPsKw7_NHZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to help with one hot key encoding with pandas. Reproduced from\n",
        "# https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n",
        "def encode_and_bind(original_dataframe, feature_to_encode):\n",
        "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    res = res.drop([feature_to_encode], axis=1)\n",
        "    return(res)"
      ],
      "metadata": {
        "id": "7bplaElohKpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing copy module\n",
        "import copy\n",
        "# Create copies of the features\n",
        "X_train_enc = copy.deepcopy(X_train);\n",
        "X_test_enc = copy.deepcopy(X_test);\n",
        "\n",
        "# Get one hot key encoding for sex attribute\n",
        "X_train_enc = encode_and_bind(X_train_enc, 'Sex')\n",
        "X_test_enc = encode_and_bind(X_test_enc, 'Sex')\n",
        "\n",
        "# Handle all other categorical variables using ordinal encoder\n",
        "\n",
        "\n",
        "# Apply normalization to numerical variables\n",
        "#num_preprocesssor = MinMaxScaler()\n",
        "#X_train_enc[num_features] = num_preprocesssor.fit_transform(X_train_enc[num_features])\n",
        "#X_test_enc[num_features] = num_preprocesssor.transform(X_test_enc[num_features])"
      ],
      "metadata": {
        "id": "2-M4IXPYLdwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the columns\n",
        "X_train_enc.columns"
      ],
      "metadata": {
        "id": "1nk2_L7gC_sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfrom the labels using the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
      ],
      "metadata": {
        "id": "UdzG4C3_Dvt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform for the target labels - attention! LabelEncoder is for target variables only!\n",
        "\n",
        "# Transform the training data\n",
        "Y_train_enc =\n",
        "# Transform the testing data\n",
        "Y_test_enc ="
      ],
      "metadata": {
        "id": "RlREK33sDsmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees\n",
        "=====\n",
        "Decision trees are considered Â¨white boxÂ¨ models allowing to completely understand how a decision was made by the algorithm. Here, we are going to use then to predict whether a patient has or is at risk of heart disease."
      ],
      "metadata": {
        "id": "TXgUGVIPZR8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a decision tree model and apply it to the testing data. Evaluate the performance of the model in terms of precision, recall, f1 score and roc auc. Moreover, calculate and visualize the confusion matrix. Useful: [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier), [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)"
      ],
      "metadata": {
        "id": "BIOOzZ8BygLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "fqaKjKby75au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the decision tree model to be used\n",
        "tree_model = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
        "\n",
        "# Fit the model to the training data\n",
        "\n",
        "\n",
        "# Apply the trained model to the testing data\n",
        "Y_tree1 =\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_tree1 = confusion_matrix(Y_test_enc, Y_tree1)\n",
        "# Plot the confusion matrix\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm_tree1, display_labels = ['Healthy', 'Diseased'])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "display.plot(ax=ax, xticks_rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QhDvAkNPykU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now make a change. Let's change the balanced weight and other parameters"
      ],
      "metadata": {
        "id": "RE0u2C3FsQwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the [graphviz](https://graphviz.readthedocs.io/en/stable/index.html) library that allows us to visualize graphs"
      ],
      "metadata": {
        "id": "zaJjSzH4UcqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz"
      ],
      "metadata": {
        "id": "DWA430xDyowX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the tree in DOT format. You can akso save it in a file with filename tree1.pdf. Useful: [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn-tree-export-graphviz)"
      ],
      "metadata": {
        "id": "wK0tjrUyyrxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(tree_model, out_file=None,\n",
        "                     feature_names=X_train_enc.columns,\n",
        "                     class_names=['Healthy', 'Diseased'],\n",
        "                     filled=True, rounded=True,\n",
        "                   special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "# Render the graph\n",
        "graph\n",
        "# Save the graph\n",
        "#graph.render(\"tree1\")\n"
      ],
      "metadata": {
        "id": "2EHaw2sXyvYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also automatically download the created file"
      ],
      "metadata": {
        "id": "7lNVD49zU6HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also download the file for better visualization\n",
        "from google.colab import files\n",
        "files.download(\"tree1.pdf\")"
      ],
      "metadata": {
        "id": "v7W1l4DDU3mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the most important features. Add them in an array in descending order of importance and create a relevant plot with the feature importance. Useful: feature_importances_, feature_names_in_ (check out [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) attributes for more details), [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)"
      ],
      "metadata": {
        "id": "997S71riyy4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the feature in descending order of importance\n",
        "sorted_idx = tree_model.feature_importances_.argsort()\n",
        "# Create the plot\n",
        "plt.barh(tree_model.feature_names_in_[sorted_idx], tree_model.feature_importances_[sorted_idx])\n",
        "plt.xlabel(\"Feature Importance for model's decision\");"
      ],
      "metadata": {
        "id": "5tOsse0By0KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests\n",
        "======================"
      ],
      "metadata": {
        "id": "TFARD0GxzY_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests are popular enseble examples of decision trees based on bagging. Let's see how they work in our problem. Useful: [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
      ],
      "metadata": {
        "id": "w-IcPtTltjAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "M65GkLY71-Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the random forest model to be used\n",
        "\n",
        "\n",
        "# Fit the model to the training data\n",
        "\n",
        "\n",
        "# Apply the trained model to the testing data to make predictions on the unseen data\n",
        "\n",
        "\n",
        "# Evaluate the performance of the model\n"
      ],
      "metadata": {
        "id": "yEbBnuvs0hY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the plot with the most important features for the classifier's decision"
      ],
      "metadata": {
        "id": "tFZ0t3GiRURa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the feature in descending order of importance\n",
        "\n",
        "# Create the plot\n"
      ],
      "metadata": {
        "id": "MMWDPE65Rdeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize one of the trees of the model, eg. the first one. Useful: estimators_ attribute from the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier) object"
      ],
      "metadata": {
        "id": "DLRYVALWbyjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Render the graph\n",
        "\n",
        "# Save the graph\n",
        "# graph.render(\"RF\")"
      ],
      "metadata": {
        "id": "TgUjoEbob1fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with different number of trees, depths and criteria for splitting. Do your results change?"
      ],
      "metadata": {
        "id": "U54SnaupTnMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost\n",
        "========\n"
      ],
      "metadata": {
        "id": "jmMWm4V9zd9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is a powerful ensemble algorithm based on decision trees and boosting."
      ],
      "metadata": {
        "id": "H1ttFdydxu3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules from the [xgboost library](https://xgboost.readthedocs.io/en/stable/)."
      ],
      "metadata": {
        "id": "NlRiZUSE4PmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier, plot_tree, to_graphviz"
      ],
      "metadata": {
        "id": "0Qmi9XJ_zYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a XGBoost model and test it. Useful: XGBClassifier. XGBClassifier\n",
        "has a lot of parameters but the most important are considered:\n",
        "(the text below is reproduced from https://gist.github.com/pb111/cc341409081dffa5e9eaf60d79562a03):-\n",
        "\n",
        "\"1. learning_rate - It gives us the step size shrinkage which is used to prevent overfitting. Its range is [0,1].\n",
        "2. max_depth - It determines how deeply each tree is allowed to grow during any boosting round.\n",
        "3. subsample - It determines the percentage of samples used per tree. Low value of subsample can lead to underfitting.\n",
        "4. colsample_bytree - It determines the percentage of features used per tree. High value of it can lead to overfitting.\n",
        "5. n_estimators - It is the number of trees we want to build.\n",
        "6. objective - It determines the loss function to be used in the process. For example, reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\n",
        "\n",
        "XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple models. These regularization parameters are as follows:-\n",
        "1. gamma - It controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. It is supported only for tree-based learners.\n",
        "2. alpha - It gives us the L1 regularization on leaf weights. A large value of it leads to more regularization.\n",
        "3. lambda - It gives us the L2 regularization on leaf weights and is smoother than L1 regularization.\""
      ],
      "metadata": {
        "id": "RYSHvsjp4Q3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "xgb_model = XGBClassifier(n_estimators = 10, max_depth = 6)\n",
        "\n",
        "# Fit the model to the training data\n",
        "\n",
        "# Apply the trained model to the testing data\n",
        "\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "\n",
        "\n",
        "# Create confusion matrix\n"
      ],
      "metadata": {
        "id": "kLEgeczj4RQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the feature in descending order of importance\n",
        "\n",
        "# Create the plot\n"
      ],
      "metadata": {
        "id": "CUBE1gORlImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "4HMyfTIbQy3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format = 'png'\n",
        "graph = to_graphviz(xgb_model, num_trees=0)\n",
        "graph\n",
        "# graph.graph_attr = {'dpi':'400'}\n",
        "#graph.render('filename', format = format)"
      ],
      "metadata": {
        "id": "fqwbqshpQYPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check whether preprocessing has any impact"
      ],
      "metadata": {
        "id": "h7QJIkW7lI9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to further improve your classification pipeline: Handling of imbalancing, Preprocessing & Automated hyper-parameter tuning\n",
        "=====================\n"
      ],
      "metadata": {
        "id": "EQAQ03h6zJdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will discuss some ways to improve your classification pipeline:\n",
        "\n",
        "1. First, we will address imbalanced data. Class imbalance, where the number of samples from one class is higher than that of the other, is a common issue in the medical domain. The class with the higher amount of data is commonly referred to as the majority class, while the class with the smaller amount of data is called the minority class. Imbalanced data require special measures to be taken; otherwise, we will end up with a model that does not generalize well. To address this, we will utilize pipeline from the imbalanced-learn library.\n",
        "\n",
        "2. Second, we will perform additional preprocessing steps, including scaling and encoding. We will use pipeline and ColumnTransformer to apply different techniques to different columns, such as numerical data and categorical data, including the 'sex' attribute.\n",
        "\n",
        "3. Finally, we will incorporate an option for automated hyperparameter tuning using grid search.\n",
        "\n",
        " Useful: [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn-compose-columntransformer)"
      ],
      "metadata": {
        "id": "3nPWgtd3y-3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Downsampling\n",
        "sampling = RandomUnderSampler(random_state=0)\n",
        "\n",
        "# Use column transformer to perform a different pipeline for categorical features, sex and numeric\n",
        "# Encoding and imputation depending on the type of data, i.e. categorical or numeric\n",
        "t = [('cat', OrdinalEncoder(), categorical_idx), ('sex', OneHotEncoder(), sex_idx), ('num', MinMaxScaler(), numerical_idx)]\n",
        "preprocess = ColumnTransformer(transformers=t)\n",
        "\n",
        "# Define the classifier\n",
        "rf = RandomForestClassifier(100, random_state=42)\n",
        "\n",
        "model = Pipeline([\n",
        "        ('sampling', sampling),\n",
        "        ('preprocessing', preprocess),\n",
        "        ('clf', rf)\n",
        "    ])\n",
        "\n",
        "#============================== Performing GridSearch ==========================\n",
        "# Set params to explore\n",
        "# max_features: features considered for the random subset, min_samples_split: The minimum number of samples required to split an internal node\n",
        "params = {\"clf__n_estimators\": [5, 10, 100], \"clf__max_depth\": [9, 5, 3, 2, None], \"clf__min_samples_split\": [2, 3, 5, 7, 10], \"clf__bootstrap\": [True]}\n",
        "grid = GridSearchCV(model, params, scoring='balanced_accuracy')\n",
        "grid.fit(X_train, Y_train_enc)\n",
        "#===============================================================================\n",
        "\n",
        "# Train the model without using gridsearch\n",
        "# model.fit(X_train, Y_train_enc)\n",
        "\n",
        "# Apply the trained model to the testing dat\n",
        "Y_rf_grid = grid.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print(classification_report(Y_test_enc, Y_rf_grid))\n",
        "# Balanced accuracy\n",
        "print(\"Balanced accuracy\", balanced_accuracy_score(Y_test_enc, Y_rf_grid))"
      ],
      "metadata": {
        "id": "QW__rI7ISqk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the grid search results"
      ],
      "metadata": {
        "id": "Uminkfy2vFAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Grid Search Results:\" )\n",
        "print(\"Best estimator:\\n\",grid.best_estimator_)\n",
        "print(\"Best score achieved by best estimator:\\n\",grid.best_score_)\n",
        "print(\"Best parameters across the searched parameters:\\n\",grid.best_params_)"
      ],
      "metadata": {
        "id": "LmXJgyqTspzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}