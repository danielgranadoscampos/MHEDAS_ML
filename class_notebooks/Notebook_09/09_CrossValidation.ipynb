{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Goal\n",
    "=====\n",
    "\n",
    "In this session we will focus mainly on the cross-validation with several splitting strategies. \n",
    "\n",
    "Dataset Information\n",
    "=====================\n",
    "\n",
    "In this example, we use the iris dataset. We split the data into a train and test dataset.\n",
    "\n"
   ],
   "metadata": {
    "id": "YRL-n3UOImHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset\n",
    "======\n"
   ],
   "metadata": {
    "id": "97udEyKQmc44"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Load the libraries and data",
   "metadata": {
    "id": "2qdB_CZ9mfC2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y\n",
    "\n"
   ],
   "metadata": {
    "id": "8dNeOcLBNZHI",
    "ExecuteTime": {
     "end_time": "2025-03-26T11:30:36.464203Z",
     "start_time": "2025-03-26T11:30:36.442284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:35:05.359196Z",
     "start_time": "2025-03-26T11:35:05.340484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# A Support Vector Machine is used as a classifier method.\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "cross_validate function and multiple metric evaluation\n",
    "======"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:41:20.998452Z",
     "start_time": "2025-03-26T11:41:20.935752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scoring = ['recall_micro','precision_micro','recall_macro','precision_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "# cv is the number of folds or the splitting approach. Default value is 5.\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=5)\n",
    "sorted(scores.keys())\n",
    "# Scores for the 5 folds.\n",
    "scores\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.       , 0.       , 0.       , 0.       , 0.0156312]),\n",
       " 'score_time': array([0.00852942, 0.        , 0.01552677, 0.        , 0.        ]),\n",
       " 'test_recall_micro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ]),\n",
       " 'test_precision_micro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ]),\n",
       " 'test_recall_macro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ]),\n",
       " 'test_precision_macro': array([0.96969697, 1.        , 0.96969697, 0.96969697, 1.        ])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:41:32.487669Z",
     "start_time": "2025-03-26T11:41:32.469458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggregation metric: mean and std\n",
    "np.mean(scores['test_recall_micro']), np.std(scores['test_recall_micro'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9800000000000001, 0.016329931618554516)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross-Validation data splitting iterators\n",
    "=====\n",
    "k-fold\n",
    "=====\n",
    "KFold divides all the samples in groups of samples, called folds (if, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using folds, and the fold left out is used for test.\n",
    "\n",
    "Example of 2-fold cross-validation on a dataset with 4 samples:"
   ],
   "metadata": {
    "id": "FViv_q9oZN0c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "\n",
    "X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "# Fold 2\n",
    "X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]"
   ],
   "metadata": {
    "id": "z3ozwWYUMZ8t",
    "ExecuteTime": {
     "end_time": "2025-03-26T11:27:37.894406Z",
     "start_time": "2025-03-26T11:27:37.886091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Stratified k-fold\n",
    "====\n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "\n",
    "Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with KFold."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import numpy as np\n",
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "for train, test in kf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Leave One Out (LOO)\n",
    "====\n",
    "LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for samples, we have different training sets and different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [1, 2, 3, 4]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ],
   "metadata": {
    "id": "k_yWkEZcDbFu",
    "ExecuteTime": {
     "end_time": "2025-03-25T17:00:46.489331Z",
     "start_time": "2025-03-25T17:00:46.473196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [0]\n",
      "[0 2 3] [1]\n",
      "[0 1 3] [2]\n",
      "[0 1 2] [3]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Group k-fold\n",
    "====\n",
    "GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjectswith several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations.\n",
    "\n",
    "Imagine you have three subjects, each with an associated number from 1 to 3:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:01:32.899431Z",
     "start_time": "2025-03-25T17:01:32.879093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "for train, test in gkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "StratifiedGroupKFold\n",
    "====\n",
    "StratifiedGroupKFold is a cross-validation scheme that combines both StratifiedKFold and GroupKFold. The idea is to try to preserve the distribution of classes in each split while keeping each group within a single split. That might be useful when you have an unbalanced dataset so that using just GroupKFold might produce skewed splits.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:02:41.892972Z",
     "start_time": "2025-03-25T17:02:41.872126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "X = list(range(18))\n",
    "y = [1] * 6 + [0] * 12\n",
    "groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\n",
    "sgkf = StratifiedGroupKFold(n_splits=3)\n",
    "for train, test in sgkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n",
      "[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n",
      "[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Leave One Group Out\n",
    "====\n",
    "LeaveOneGroupOut is a cross-validation scheme where each split holds out samples belonging to one specific group. Group information is provided via an array that encodes the group of each sample.\n",
    "\n",
    "Each training set is thus constituted by all the samples except the ones related to a specific group. This is the same as LeavePGroupsOut with n_groups=1 and the same as GroupKFold with n_splits equal to the number of unique labels passed to the groups parameter.\n",
    "\n",
    "For example, in the cases of multiple experiments, LeaveOneGroupOut can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:03:38.930558Z",
     "start_time": "2025-03-25T17:03:38.916067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X = [1, 5, 10, 50, 60, 70, 80]\n",
    "y = [0, 1, 1, 2, 2, 2, 2]\n",
    "groups = [1, 1, 2, 2, 3, 3, 3]\n",
    "logo = LeaveOneGroupOut()\n",
    "for train, test in logo.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [0 1]\n",
      "[0 1 4 5 6] [2 3]\n",
      "[0 1 2 3] [4 5 6]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Cross-Validation running using a loop\n",
    " ====\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:55:11.758038Z",
     "start_time": "2025-03-26T11:55:11.729637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Splitting hold-out for model performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "i = 1\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X_train,y_train):\n",
    "    X_trainFold = X_train[train_index]\n",
    "    X_testFold = X_train[test_index]\n",
    "    y_trainFold = y_train[train_index]\n",
    "    y_testFold = y_train[test_index]\n",
    "        \n",
    "    #Train the model\n",
    "    clf.fit(X_trainFold, y_trainFold) #Training the model\n",
    "    print(f\"Accuracy for the fold no. {i} on the validation set: {accuracy_score(y_testFold, clf.predict(X_testFold))}\")\n",
    "    i += 1\n",
    "\n",
    "# Hold-out validation\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Accuracy for the hold-out: {accuracy_score(y_test, clf.predict(X_test))}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the fold no. 1 on the validation set: 0.9583333333333334\n",
      "Accuracy for the fold no. 2 on the validation set: 0.9166666666666666\n",
      "Accuracy for the fold no. 3 on the validation set: 1.0\n",
      "Accuracy for the fold no. 4 on the validation set: 1.0\n",
      "Accuracy for the fold no. 5 on the validation set: 0.875\n",
      "Accuracy for the hold-out: 1.0\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ]
}
