{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cae9e0",
   "metadata": {},
   "source": [
    "## Results Section Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0a7ba",
   "metadata": {},
   "source": [
    "This notebook provides a structured workflow to generate the tables, descriptive statistics, plots, and statistical tests needed for the Results section of your ML report. \n",
    "\n",
    "**Instructions:**\n",
    "- Place your metrics file (`metrics.csv`) and prediction file (`predictions.csv`) in the same folder as this notebook.\n",
    "- The `metrics.csv` file should contain: `fold`, `Model`, `Metric1`, `Metric2`, `Metric3`.\n",
    "- The `predictions.csv` file should contain: `fold`, `Model`, `y_true`, `y_pred`, `y_score` for ROC curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce617c3",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "metrics = pd.read_csv('metrics.csv')\n",
    "predictions = pd.read_csv('predictions.csv')\n",
    "# Peek at the data\n",
    "display(metrics.head())\n",
    "display(predictions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f6bfa",
   "metadata": {},
   "source": [
    "### Descriptive Statistics (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics for each metric and model\n",
    "desc = metrics.groupby('Model').agg({\n",
    "    'Metric1': ['mean', 'std'],\n",
    "    'Metric2': ['mean', 'std'],\n",
    "    'Metric3': ['mean', 'std']\n",
    "})\n",
    "desc.columns = ['_'.join(col) for col in desc.columns]\n",
    "desc.reset_index(inplace=True)\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2a9b1",
   "metadata": {},
   "source": [
    "### Confusion Matrices (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e454f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each model\n",
    "models = predictions['Model'].unique()\n",
    "for model in models:\n",
    "    df = predictions[predictions['Model'] == model]\n",
    "    cm = confusion_matrix(df['y_true'], df['y_pred'])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f'Confusion Matrix: {model}')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "    plt.savefig(f'confusion_matrix_{model}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53d409",
   "metadata": {},
   "source": [
    "### Boxplots of Metric Distributions (Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for each metric\n",
    "for metric in ['Metric1', 'Metric2', 'Metric3']:\n",
    "    plt.figure()\n",
    "    metrics.boxplot(column=metric, by='Model', grid=False)\n",
    "    plt.title(f'Boxplot of {metric} by Model')\n",
    "    plt.suptitle('')  # remove automatic title\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.savefig(f'boxplot_{metric}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bb244",
   "metadata": {},
   "source": [
    "### Statistical Tests (p-values & Confidence Intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: paired t-test and 95% CI for Metric1 between two models\n",
    "model_pairs = [('Method1', 'Method2')]\n",
    "results = []\n",
    "for m1, m2 in model_pairs:\n",
    "    d1 = metrics[metrics['Model'] == m1]['Metric1']\n",
    "    d2 = metrics[metrics['Model'] == m2]['Metric1']\n",
    "    # Paired t-test\n",
    "    t_stat, p_val = stats.ttest_rel(d1, d2)\n",
    "    # 95% CI for difference\n",
    "    diff = d1 - d2\n",
    "    ci_low, ci_upp = DescrStatsW(diff).tconfint_mean()\n",
    "    results.append({'Comparison': f'{m1} vs {m2}', 'p-value': p_val, 'CI_low': ci_low, 'CI_up': ci_upp})\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c352c",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each model\n",
    "plt.figure()\n",
    "for model in models:\n",
    "    df = predictions[predictions['Model'] == model]\n",
    "    fpr, tpr, _ = roc_curve(df['y_true'], df['y_score'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('roc_curves.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
